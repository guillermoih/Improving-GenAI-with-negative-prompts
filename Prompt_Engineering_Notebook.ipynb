{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T09:46:06.509240Z",
     "start_time": "2024-04-11T09:46:03.370825Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/COW_66/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json, requests, time, textwrap, csv\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from diffusers import StableDiffusionXLPipeline, StableDiffusionPipeline, EulerDiscreteScheduler\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T09:46:07.317394Z",
     "start_time": "2024-04-11T09:46:06.512322Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"popular\") # required to download the stopwords lists\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T09:46:31.820922Z",
     "start_time": "2024-04-11T09:46:07.320662Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 6/6 [00:09<00:00,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "# Stable Difussion XL\n",
    "#STABLE_DIFF_PIPELINE = StableDiffusionXLPipeline.from_pretrained(\n",
    "#    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    "#)\n",
    "#STABLE_DIFF_PIPELINE = STABLE_DIFF_PIPELINE.to(\"cuda\")\n",
    "\n",
    "# Stable Difussion v1\n",
    "# STABLE_DIFF_PIPELINE = StableDiffusionPipeline.from_pretrained(\n",
    "#     \"CompVis/stable-diffusion-v1-1\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    "# )\n",
    "# STABLE_DIFF_PIPELINE = STABLE_DIFF_PIPELINE.to(\"cuda\")\n",
    "\n",
    "# Stable Difussion v2\n",
    "model_id = \"stabilityai/stable-diffusion-2\"\n",
    "scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "STABLE_DIFF_PIPELINE = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id, scheduler=scheduler, torch_dtype=torch.float16\n",
    ")\n",
    "STABLE_DIFF_PIPELINE = STABLE_DIFF_PIPELINE.to(\"cuda\")\n",
    "\n",
    "# BLIP\n",
    "BLIP_PROCCESOR = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\", max_length=500)\n",
    "BLIP_MODEL = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\", max_length=500)\n",
    "\n",
    "# MPNET embedding\n",
    "MPNET = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T09:46:31.828642Z",
     "start_time": "2024-04-11T09:46:31.823978Z"
    }
   },
   "outputs": [],
   "source": [
    "# Caption generation using Blip\n",
    "def generate_caption(img):\n",
    "    inputs = BLIP_PROCCESOR(img, return_tensors=\"pt\")\n",
    "\n",
    "    out = BLIP_MODEL.generate(**inputs)\n",
    "    return BLIP_PROCCESOR.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T09:46:31.835104Z",
     "start_time": "2024-04-11T09:46:31.831136Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate captions for a batch of images\n",
    "def get_batch_captions(images):\n",
    "    captions = []\n",
    "    for i, img in enumerate(tqdm(images, desc=\"Generating captions\")):\n",
    "        caption = generate_caption(img)\n",
    "        captions.append(caption)\n",
    "        \n",
    "    return captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T09:46:31.840889Z",
     "start_time": "2024-04-11T09:46:31.837186Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cosine similarity distance\n",
    "def cosine_sim(a, b):\n",
    "    return  np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T09:46:31.847309Z",
     "start_time": "2024-04-11T09:46:31.842932Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using cosine distance between text embeddings of MPNet\n",
    "def get_batch_similarities(prompt, captions):\n",
    "    capt_em = MPNET.encode(captions)\n",
    "    prompt_em = MPNET.encode(prompt)\n",
    "\n",
    "    similarities = []\n",
    "\n",
    "    for cap in tqdm(capt_em, desc=\"Calculating similarities\"):\n",
    "        similarities.append(cosine_sim(cap, prompt_em))\n",
    "    \n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T09:46:31.856443Z",
     "start_time": "2024-04-11T09:46:31.851727Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_similarity(prompt, caption):\n",
    "    prompt_em = MPNET.encode(prompt)\n",
    "    capt_em = MPNET.encode(caption)\n",
    "    \n",
    "    return cosine_sim(prompt_em, capt_em)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asses which words produce a negative impact in the similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:53:11.648512Z",
     "start_time": "2024-04-11T12:53:11.641514Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_neg_pos_words(prompt, captions):\n",
    "    neg_words = []\n",
    "    pos_words = []\n",
    "    for caption in captions:\n",
    "        original_sim = get_similarity(prompt, caption)\n",
    "        for word in caption.split():\n",
    "            # Test each caption without every word combination (if it isnt a stop word)\n",
    "            if word not in STOPWORDS:\n",
    "                new_cap = caption.replace(word, '')\n",
    "                similarity = get_similarity(prompt, new_cap)\n",
    "\n",
    "                if original_sim-similarity < -(original_sim*0.3):\n",
    "                    neg_words.append(word)\n",
    "                elif original_sim-similarity > original_sim*0.3:\n",
    "                    pos_words.append(word)\n",
    "                \n",
    "    return neg_words, pos_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:53:12.097287Z",
     "start_time": "2024-04-11T12:53:12.086896Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_results(images, captions, similarities, prompt=None, path=None):\n",
    "    fig, ax = plt.subplots(1, len(captions), figsize=(5*len(captions), 6))\n",
    "    i=0\n",
    "\n",
    "    for img, caption, similarity in zip(images, captions, similarities):\n",
    "        ax[i].imshow(img)\n",
    "        ax[i].set_title(textwrap.fill(caption, 40))\n",
    "        ax[i].set_xlabel('Similarity ' + str(similarity))\n",
    "        ax[i].set_xticks([])\n",
    "        ax[i].set_yticks([])\n",
    "\n",
    "        i+=1\n",
    "        \n",
    "    if len(images) == 3:\n",
    "        ax[0].set_xlabel('Worst image' + '\\nSimilarity ' + str(similarities[0]))\n",
    "        ax[1].set_xlabel('Best image' + '\\nSimilarity ' + str(similarities[1]))\n",
    "        ax[2].set_xlabel('Base image' + '\\nSimilarity ' + str(similarities[2]))\n",
    "    \n",
    "    if prompt!=None:\n",
    "        fig.suptitle('Prompt:' + textwrap.fill(prompt, 100))\n",
    "        \n",
    "    if path!=None:\n",
    "        plt.savefig(path)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:53:12.431453Z",
     "start_time": "2024-04-11T12:53:12.428121Z"
    }
   },
   "outputs": [],
   "source": [
    "IT_GEN_IMGS = 5\n",
    "n_gens = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:53:12.588732Z",
     "start_time": "2024-04-11T12:53:12.583717Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set guidance scale value depending on average fitness.\n",
    "# It will define a range between 5 and 9 depending on the fitness, with a max\n",
    "# fitness of 0.6 and a min fitness of 0.2\n",
    "def get_guidance(fit):\n",
    "    if fit >= 0.2 and fit <= 0.6:\n",
    "        new_fit = (fit-0.2)/(0.6-0.2)\n",
    "        new_fit = 7+new_fit*(13-7)\n",
    "        return new_fit\n",
    "    elif fit < 0.2:\n",
    "        return 7\n",
    "    else:\n",
    "        return 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:53:12.930972Z",
     "start_time": "2024-04-11T12:53:12.900048Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def optimization(prompt, original_img, n_gens, csv_writer, idx, res_folder):\n",
    "    # Initial ttention to the caption\n",
    "    guid_scale = 7\n",
    "\n",
    "    # Save all generated images\n",
    "    total_imgs = []\n",
    "    total_captions = []\n",
    "    total_similarities = []\n",
    "    \n",
    "    negative_words = ''\n",
    "    positive_words = ''\n",
    "    negative_words_list = ''\n",
    "    positive_words_list = ''\n",
    "    \n",
    "    # Save evolution of similarity within optimization process\n",
    "    it_mean_sim = []\n",
    "    it_max_sim = []\n",
    "    it_min_sim = []\n",
    "    \n",
    "    # Save negative and positive word list for each iteration\n",
    "    it_neg_words = []\n",
    "    it_pos_words = []\n",
    "    \n",
    "    it_neg_words_num = []\n",
    "    it_pos_words_num = []\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    while i<n_gens:\n",
    "        images_batch = []\n",
    "        captions_batch = []\n",
    "        similarities_batch = []\n",
    "\n",
    "        print(\"Guidance\", guid_scale)\n",
    "        # Generating original batch of images\n",
    "        images_batch = STABLE_DIFF_PIPELINE(prompt=prompt, num_images_per_prompt=IT_GEN_IMGS,\n",
    "                                            negative_prompt=negative_words_list,\n",
    "                                            #prompt_2=positive_words_list,\n",
    "                                            guidance_scale=guid_scale, output_type='pil').images\n",
    "        captions_batch = get_batch_captions(images_batch)\n",
    "        similarities_batch = get_batch_similarities(prompt, captions_batch)\n",
    "\n",
    "        #show_results(images_batch, captions_batch, similarities_batch)\n",
    "\n",
    "        # Get the similarity value of the iteration\n",
    "        mean_sim = np.mean(similarities_batch)\n",
    "\n",
    "        it_mean_sim.append(mean_sim)\n",
    "        it_max_sim.append(np.max(similarities_batch))\n",
    "        it_min_sim.append(np.min(similarities_batch))\n",
    "\n",
    "        # Add the new batch of samples to the total\n",
    "        total_imgs.extend(images_batch)\n",
    "        total_captions.extend(captions_batch)\n",
    "        total_similarities.extend(similarities_batch)\n",
    "\n",
    "        print('-'*5, 'Mean similarity of iteration', i, '=', mean_sim)\n",
    "\n",
    "        guid_scale = get_guidance(mean_sim)\n",
    "\n",
    "        # Get words that produces negative similarity\n",
    "        neg_words, pos_words = get_neg_pos_words(prompt, captions_batch)\n",
    "\n",
    "        # Add new words to the string\n",
    "        neg_words = ' '.join(neg_words)\n",
    "        pos_words = ' '.join(pos_words)\n",
    "        negative_words = negative_words + ' ' + neg_words\n",
    "        positive_words = positive_words + ' ' + pos_words\n",
    "        # Delete duplicates\n",
    "        negative_words = ' '.join(list(set(negative_words.split(' '))))\n",
    "        positive_words = ' '.join(list(set(positive_words.split(' '))))\n",
    "        # Add commas between words\n",
    "        negative_words_list = negative_words.replace(\" \", \", \")[2:]\n",
    "        positive_words_list = positive_words.replace(\" \", \", \")[2:]\n",
    "        print('Positive words list:', positive_words_list)\n",
    "        print('Negative words list:', negative_words_list)\n",
    "        \n",
    "        # Save word list and number of words\n",
    "        it_neg_words.append(negative_words)\n",
    "        it_pos_words.append(positive_words)\n",
    "        \n",
    "        it_neg_words_num.append(len(negative_words.split(' ')) - 1)\n",
    "        it_pos_words_num.append(len(positive_words.split(' ')) - 1)\n",
    "        \n",
    "        i+=1\n",
    "\n",
    "    # Order samples by their similarity\n",
    "    zip_list = zip(total_imgs, total_captions, total_similarities, it_neg_words, it_pos_words)\n",
    "    sort_zip_list = sorted(zip_list, key=lambda x: x[2], reverse=True)\n",
    "    sort_total_imgs, sort_total_captions, sort_total_similarities, sort_neg_words, sort_pos_words = zip(*sort_zip_list)\n",
    "\n",
    "    # Show all results ordered\n",
    "    #show_results(sort_total_imgs, sort_total_captions, sort_total_similarities)\n",
    "\n",
    "    # Get best result\n",
    "    best_img = sort_total_imgs[0]\n",
    "    best_caption = sort_total_captions[0]\n",
    "    best_similarity = sort_total_similarities[0]\n",
    "    best_neg_words = sort_neg_words[0]\n",
    "    best_pos_words = sort_pos_words[0]\n",
    "\n",
    "    worst_img = sort_total_imgs[-1]\n",
    "    worst_caption = sort_total_captions[-1]\n",
    "    worst_similarity = sort_total_similarities[-1]\n",
    "    worst_neg_words = sort_neg_words[-1]\n",
    "    worst_pos_words = sort_pos_words[-1]\n",
    "\n",
    "    # Get caption and similarity of original image\n",
    "    original_caption = generate_caption(original_img)\n",
    "    original_similarity = get_similarity(prompt, original_caption)\n",
    "\n",
    "    show_results([worst_img, best_img, original_img],\n",
    "                 [worst_caption, best_caption, original_caption],\n",
    "                 [worst_similarity, best_similarity, original_similarity],\n",
    "                 prompt = prompt,\n",
    "                 path=res_folder + \"{:03d}\".format(idx) + '_images.png')\n",
    "\n",
    "    best_img.save(res_folder + \"{:03d}\".format(idx) + '_best.png')\n",
    "    worst_img.save(res_folder + \"{:03d}\".format(idx) + '_worst.png')\n",
    "    original_img.save(res_folder + \"{:03d}\".format(idx) + '_base.png')\n",
    "\n",
    "    # Show evolution of similarity\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    plt.suptitle('Similarity evolution with the prompt')\n",
    "    \n",
    "    color = 'tab:blue'\n",
    "    ax.plot(it_mean_sim, color=color)\n",
    "    ax.fill_between(np.arange(len(it_mean_sim)), \n",
    "                     np.array(it_min_sim),\n",
    "                     np.array(it_max_sim),\n",
    "                     color=color, alpha=0.2)\n",
    "    # Original similarity\n",
    "    ax.axhline(y=original_similarity, color='k', linestyle='dashed', linewidth=0.7)\n",
    "\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Similarity', color=color)\n",
    "    \n",
    "    ax2 = ax.twinx()\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Neg words number', color=color)\n",
    "    ax2.plot(it_neg_words_num, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    plt.savefig(res_folder + \"{:03d}\".format(idx) + '_similarities.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Write in the .csv\n",
    "    csv_writer.writerow({'id' : \"{:03d}\".format(idx),\n",
    "                         'prompt': prompt,\n",
    "                         'neg_words': negative_words_list,\n",
    "                         'pos_words': positive_words_list,\n",
    "                         'best_caption': best_caption, 'best_sim': best_similarity,\n",
    "                         'best_neg_words': best_neg_words, 'best_pos_words': best_pos_words,\n",
    "                         'worst_caption': worst_caption, 'worst_sim': worst_similarity,\n",
    "                         'worst_neg_words': worst_neg_words, 'worst_pos_words': worst_pos_words,\n",
    "                         'base_caption': original_caption, 'base_sim': original_similarity,\n",
    "                         'sim_hist': str(it_mean_sim), 'min_sim_hist': str(it_min_sim), 'max_sim_hist': str(it_max_sim),\n",
    "                         'neg_words_hist': str(it_neg_words), 'pos_words_hist': str(it_pos_words),\n",
    "                         'neg_words_num_hist': str(it_neg_words_num), 'pos_words_num_hist': str(it_pos_words_num)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download DiffusionDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:53:16.616666Z",
     "start_time": "2024-04-11T12:53:13.221415Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset with the `large_random_1k` subset\n",
    "dataset = load_dataset('poloclub/diffusiondb', 'large_random_1k')\n",
    "data = dataset['train']\n",
    "\n",
    "# Delete duplicates\n",
    "prompts_list = list(set(data['prompt']))\n",
    "\n",
    "# Delete prompts with commas, 'and' o less than 5 words and more than 25\n",
    "prompts = []\n",
    "for prompt in prompts_list:\n",
    "    if len(prompt.split()) >= 5 and len(prompt.split()) <= 25:\n",
    "        if ',' not in prompt and 'and' not in prompt:\n",
    "            prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T12:53:16.673258Z",
     "start_time": "2024-04-11T12:53:16.667758Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a cell shaded cartoon still of walter white ',\n",
       " '( steve jobs ) gold king tut mask ',\n",
       " 'steve jobs crossing the alps painting by jacques louis david. ',\n",
       " 'tom waits as the goblin king by brian froud ',\n",
       " 'honey label by adolphe mucha ',\n",
       " 'turn of the century sepia photo of a man waiting at the train station while using an ipad ',\n",
       " 'astral projection by gustave dore ',\n",
       " 'huge glitter bomb explosion above city ',\n",
       " 'a capybara fighting an alien ',\n",
       " 'turin in 2 0 7 0 ',\n",
       " 'the most amazing smore you have ever seen ',\n",
       " 'steve jobs breaks the tablets of the law by gustave dore. ',\n",
       " 'funko pop of hillary clinton ',\n",
       " 'book cover for the epic fantasy book titled echos of dragons ',\n",
       " 'painting by bridget bate tichenor ',\n",
       " 'funko pop of danny devito ',\n",
       " '3 d octane render of a glowing yellow orb with white clear wings flying ']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open results .csv\n",
    "res_folder = 'results/'\n",
    "with open(res_folder + 'csvfile.csv', 'a+', newline ='') as csv_file:\n",
    "    header = ['id', 'prompt', 'neg_words', 'pos_words',\n",
    "              'best_caption', 'best_sim', 'best_neg_words', 'best_pos_words',\n",
    "              'base_caption', 'base_sim',\n",
    "              'worst_caption', 'worst_sim', 'worst_neg_words', 'worst_pos_words',\n",
    "              'sim_hist', 'min_sim_hist', 'max_sim_hist',\n",
    "              'neg_words_hist', 'pos_words_hist',\n",
    "              'neg_words_num_hist', 'pos_words_num_hist']\n",
    "    csv_writer = csv.DictWriter(csv_file, fieldnames = header, delimiter=';')\n",
    "    csv_writer.writeheader()\n",
    "\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print('Prompt:', prompt)\n",
    "        # Get the original image\n",
    "        image = data['image'][data['prompt'].index(prompt)]\n",
    "        optimization(prompt, image, n_gens, csv_writer, i, res_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
